{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:12:51.390024Z","iopub.status.busy":"2024-05-30T10:12:51.389195Z","iopub.status.idle":"2024-05-30T10:13:45.515927Z","shell.execute_reply":"2024-05-30T10:13:45.514900Z","shell.execute_reply.started":"2024-05-30T10:12:51.389989Z"},"trusted":true},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install optree\n","!pip install anytree"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-30T10:13:45.518178Z","iopub.status.busy":"2024-05-30T10:13:45.517866Z","iopub.status.idle":"2024-05-30T10:13:59.433103Z","shell.execute_reply":"2024-05-30T10:13:59.432037Z","shell.execute_reply.started":"2024-05-30T10:13:45.518147Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n","from keras.optimizers import Adam\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import warnings\n","import os\n","import cv2\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","np.random.seed(54)\n","tf.random.set_seed(54)\n","train_dir = '/kaggle/input/Original Images/Original Images/'\n","checkpoint_filepath = '/kaggle/working/best_self_built_model.keras'"]},{"cell_type":"markdown","metadata":{},"source":["## List of used pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.435107Z","iopub.status.busy":"2024-05-30T10:13:59.434402Z","iopub.status.idle":"2024-05-30T10:13:59.448377Z","shell.execute_reply":"2024-05-30T10:13:59.446755Z","shell.execute_reply.started":"2024-05-30T10:13:59.435068Z"},"trusted":true},"outputs":[],"source":["from keras.applications import VGG16\n","from keras.applications import ResNet50\n","from keras.applications import DenseNet121\n","from keras.applications import InceptionV3\n","from keras.applications import MobileNetV2\n","from keras.applications import EfficientNetB0"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.453030Z","iopub.status.busy":"2024-05-30T10:13:59.452196Z","iopub.status.idle":"2024-05-30T10:13:59.477205Z","shell.execute_reply":"2024-05-30T10:13:59.476232Z","shell.execute_reply.started":"2024-05-30T10:13:59.452989Z"},"trusted":true},"outputs":[],"source":["image_size = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.478954Z","iopub.status.busy":"2024-05-30T10:13:59.478520Z","iopub.status.idle":"2024-05-30T10:13:59.865992Z","shell.execute_reply":"2024-05-30T10:13:59.865020Z","shell.execute_reply.started":"2024-05-30T10:13:59.478919Z"},"trusted":true},"outputs":[],"source":["# Using ImageDataGenerator for data augmentation\n","generator = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.1 \n",")\n","\n","train_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"training\"  # This is for training data\n",")\n","\n","val_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"validation\"  # This is for validation data\n",")\n","\n","# Evaluate the model on the test set\n","test_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"validation\"  # Use a portion of the data for testing\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.867842Z","iopub.status.busy":"2024-05-30T10:13:59.867189Z","iopub.status.idle":"2024-05-30T10:13:59.873255Z","shell.execute_reply":"2024-05-30T10:13:59.872226Z","shell.execute_reply.started":"2024-05-30T10:13:59.867805Z"},"trusted":true},"outputs":[],"source":["# Get the list of classes\n","classes = list(train_ds.class_indices.keys())\n","print(classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.874919Z","iopub.status.busy":"2024-05-30T10:13:59.874595Z","iopub.status.idle":"2024-05-30T10:13:59.889256Z","shell.execute_reply":"2024-05-30T10:13:59.888341Z","shell.execute_reply.started":"2024-05-30T10:13:59.874890Z"},"trusted":true},"outputs":[],"source":["def plot_roc_curves(model, test_ds, num_classes=31, batch_size=32, num_samples=1000):\n","    y_true = []\n","    y_pred = []\n","\n","    for i, (images, labels) in enumerate(test_ds):\n","        if len(y_true) >= num_samples:\n","            break\n","        y_true.extend(np.argmax(labels, axis=1))\n","        y_pred.extend(model.predict(images, batch_size=batch_size, verbose=0))\n","\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","\n","    # Binarize the true labels\n","    y_true_bin = label_binarize(y_true, classes=[i for i in range(num_classes)])\n","\n","    # Compute ROC curve and ROC area for each class\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    for i in range(num_classes):\n","        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    # Compute micro-average ROC curve and ROC area\n","    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred.ravel())\n","    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","    # Plot ROC curves\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='darkorange', lw=2,\n","             label='Micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n","\n","    # Plot ROC curve for each class\n","    for i in range(num_classes):\n","        plt.plot(fpr[i], tpr[i], lw=2, alpha=0.3,\n","                 label='Class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n","\n","    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\", bbox_to_anchor=(1.5, 0))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Set memory growth to avoid memory allocation issues\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs configured.\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)\n","else:\n","    print(\"No GPUs found. Training will use the CPU.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Build model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["IMAGE_SIZE = [image_size, image_size, 3]\n","NUM_CLASSES = 31\n","\n","model = Sequential()\n","\n","# Add convolutional layers\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=IMAGE_SIZE))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Flatten the output of the convolutional layers\n","model.add(Flatten())\n","\n","# Add fully connected layers\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","# Compile the model\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer=Adam(),\n","    metrics=['accuracy']\n",")\n","\n","# Print model summary\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Set up checkpoint for fetching best model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    monitor='train_loss',\n","    save_best_only=True,\n","    mode='min',\n","    verbose=0\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["history = model.fit(\n","    train_ds,\n","    epochs=10,\n","    validation_data=val_ds,\n","    batch_size=32,\n","    callbacks=[checkpoint_callback]\n",")\n","\n","# Load the best model after training\n","best_model = tf.keras.models.load_model(checkpoint_filepath)"]},{"cell_type":"markdown","metadata":{},"source":["## Result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n","ax = ax.ravel()\n","\n","for i, met in enumerate(['accuracy', 'loss']):\n","    ax[i].plot(history.history[met])\n","    ax[i].plot(history.history['val_' + met])\n","    ax[i].set_title('Model {}'.format(met))\n","    ax[i].set_xlabel('epochs')\n","    ax[i].set_ylabel(met)\n","    ax[i].legend(['train', 'val'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss, train_accuracy = model.evaluate(train_ds)\n","print(f\"Training Accuracy: {train_accuracy*100: .2f}\")\n","print(f\"Training Loss: {train_loss: .2f}\")\n","\n","validation_loss, validation_accuracy = model.evaluate(val_ds)\n","print(f\"Validation Accuracy: {validation_accuracy*100: .2f}\")\n","print(f\"Validation Loss: {validation_loss: .2f}\")\n","\n","test_loss, test_accuracy = model.evaluate(test_ds)\n","print(f\"Test Accuracy: {test_accuracy*100: .2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_roc_curves(model, test_ds)"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tune pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_size = 128"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Using ImageDataGenerator for data augmentation\n","generator = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.1 \n",")\n","\n","train_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"training\"  # This is for training data\n",")\n","\n","val_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"validation\"  # This is for validation data\n",")\n","\n","# Evaluate the model on the test set\n","test_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"validation\"  # Use a portion of the data for testing\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Build model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["IMAGE_SIZE = [image_size, image_size, 3]\n","\n","# Loading the weights of pretrained model without the top layer. These weights are trained on Imagenet dataset.\n","# pretrained_model = VGG16(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = ResNet50(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","pretrained_model = DenseNet121(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = InceptionV3(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = MobileNetV2(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = EfficientNetB0(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","\n","\n","# This will exclude the initial layers from the training phase as they have already been trained.\n","for layer in pretrained_model.layers:\n","    layer.trainable = False\n","\n","x = Flatten()(pretrained_model.output)\n","x = Dense(128, activation='relu')(x)  # We can add a new fully connected layer but it will increase the execution time.\n","# x = Dropout(0.15)(x)  # Add a Dropout layer with a dropout rate of 0.3\n","x = Dense(31, activation='softmax')(x)  # Adding the output layer with softmax function as this is a multi-class classification problem.\n","\n","model = Model(inputs=pretrained_model.input, outputs=x)\n","\n","# Compile the model\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")\n","\n","print(\"Model created and compiled.\")\n","\n","# Summary of the model\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Set up checkpoint for fetching best model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    monitor='train_loss',\n","    save_best_only=True,\n","    mode='min',\n","    verbose=0\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = model.fit(\n","    train_ds,\n","    epochs=30,\n","    validation_data=val_ds,\n","    batch_size=32,\n","    callbacks=[checkpoint_callback]\n",")\n","\n","best_model = tf.keras.models.load_model(checkpoint_filepath)"]},{"cell_type":"markdown","metadata":{},"source":["## Result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n","ax = ax.ravel()\n","\n","for i, met in enumerate(['accuracy', 'loss']):\n","    ax[i].plot(history.history[met])\n","    ax[i].plot(history.history['val_' + met])\n","    ax[i].set_title('Model {}'.format(met))\n","    ax[i].set_xlabel('epochs')\n","    ax[i].set_ylabel(met)\n","    ax[i].legend(['train', 'val'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss, train_accuracy = model.evaluate(train_ds)\n","print(f\"Training Accuracy: {train_accuracy*100: .2f}\")\n","print(f\"Training Loss: {train_loss: .2f}\")\n","\n","validation_loss, validation_accuracy = model.evaluate(val_ds)\n","print(f\"Validation Accuracy: {validation_accuracy*100: .2f}\")\n","print(f\"Validation Loss: {validation_loss: .2f}\")\n","\n","test_loss, test_accuracy = model.evaluate(test_ds)\n","print(f\"Test Accuracy: {test_accuracy*100: .2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_roc_curves(model, test_ds)"]},{"cell_type":"markdown","metadata":{},"source":["## Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_model.save('best_fine_tune_model.h5')\n","from IPython.display import FileLink\n","\n","# Create a download link\n","FileLink('FRM.h5')"]},{"cell_type":"markdown","metadata":{},"source":["# Cropping image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get the directory containing Haar cascade files\n","cascade_dir = cv2.data.haarcascades\n","\n","# Path to the Haar cascade file for frontal face detection\n","cascade_file = os.path.join(cascade_dir, 'haarcascade_frontalface_default.xml')\n","\n","# Check if the cascade file exists\n","if os.path.isfile(cascade_file):\n","    print(\"Haar cascade file found:\", cascade_file)\n","else:\n","    print(\"Haar cascade file not found. Downloading...\")\n","    cv2_base_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/\"\n","    cascade_url = cv2_base_url + 'haarcascade_frontalface_default.xml'\n","    os.system(f\"wget {cascade_url} -P {cascade_dir}\")\n","    print(\"Haar cascade file downloaded successfully.\")\n","\n","# Now, you can use cascade_file as the filter_path in your code.\n","filter_path = cascade_file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to detect faces and crop them from an image\n","def detect_and_crop_faces(image):\n","    face_cascade = cv2.CascadeClassifier(filter_path)\n","    faces = face_cascade.detectMultiScale(image, 1.3, 5)\n","    cropped_faces = []\n","    for (x, y, w, h) in faces:\n","        cropped_faces.append(image[y:y+h, x:x+w])\n","    return cropped_faces"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Path to the dataset directory\n","dataset_dir = \"/kaggle/input/Original Images/Original Images\"\n","# Path to store the cropped images\n","cropped_dataset_dir = \"/kaggle/working/CroppedImages\"\n","# Path to store the split train and test sets\n","train_dir = os.path.join(cropped_dataset_dir, \"train\")\n","test_dir = os.path.join(cropped_dataset_dir, \"test\")\n","\n","# Create directories for train and test sets\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(test_dir, exist_ok=True)\n","\n","# Common size to which all face images will be resized\n","common_size = (128, 128)\n","\n","# Iterate through each subdirectory (each person's folder)\n","for subdir in os.listdir(dataset_dir):\n","    subdir_path = os.path.join(dataset_dir, subdir)\n","    if os.path.isdir(subdir_path):\n","        # Create corresponding subdirectories in train and test folders\n","        train_subdir_path = os.path.join(train_dir, subdir)\n","        test_subdir_path = os.path.join(test_dir, subdir)\n","        os.makedirs(train_subdir_path, exist_ok=True)\n","        os.makedirs(test_subdir_path, exist_ok=True)\n","\n","        # Get the list of image files in the subdirectory\n","        image_files = [f for f in os.listdir(subdir_path) if f.endswith('.jpg')]\n","\n","        # Iterate through each image in the subdirectory\n","        for image_name in image_files:\n","            image_path = os.path.join(subdir_path, image_name)\n","            # Read the image\n","            img = cv2.imread(image_path)\n","            # Detect and crop faces from the image (function detect_and_crop_faces to be defined)\n","            faces = detect_and_crop_faces(img)\n","            # Resize each face to a common size before appending to the list\n","            for idx, face in enumerate(faces):\n","                if face is not None:\n","                    resized_face = cv2.resize(face, common_size)\n","                    # Decide whether to put the image in train or test set\n","                    if np.random.rand() < 0.9:  # 90% train, 10% test\n","                        save_path = os.path.join(train_subdir_path, f\"{image_name}_{idx}.jpg\")\n","                    else:\n","                        save_path = os.path.join(test_subdir_path, f\"{image_name}_{idx}.jpg\")\n","                    # Save the cropped face image\n","                    cv2.imwrite(save_path, resized_face)\n","print('dataset created')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%matplotlib inline\n","cropped_img = mpimg.imread('/kaggle/working/CroppedImages/train/Billie Eilish/Billie Eilish_1.jpg_0.jpg')\n","original_img = mpimg.imread('/kaggle/input/Original Images/Original Images/Billie Eilish/Billie Eilish_1.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["imgplot = plt.imshow(original_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["imgplot = plt.imshow(cropped_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import os\n","from anytree import Node, RenderTree\n","\n","def create_directory_tree(root_path, parent=None):\n","    \"\"\"\n","    Recursively create a directory tree structure using AnyTree.\n","    \"\"\"\n","    node = Node(os.path.basename(root_path), parent=parent)\n","    if os.path.isdir(root_path):\n","        for item in sorted(os.listdir(root_path)):\n","            item_path = os.path.join(root_path, item)\n","            create_directory_tree(item_path, parent=node)\n","\n","def print_directory_tree(root_path):\n","    \"\"\"\n","    Print the directory tree structure using AnyTree.\n","    \"\"\"\n","    root = Node(os.path.basename(root_path))\n","    create_directory_tree(root_path, root)\n","\n","# Define the root directory\n","root_dir = \"/kaggle/working/CroppedImages\"\n","\n","# Print the directory tree\n","print_directory_tree(root_dir)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:13:59.891017Z","iopub.status.busy":"2024-05-30T10:13:59.890556Z","iopub.status.idle":"2024-05-30T10:13:59.900720Z","shell.execute_reply":"2024-05-30T10:13:59.899821Z","shell.execute_reply.started":"2024-05-30T10:13:59.890981Z"},"trusted":true},"outputs":[],"source":["image_size = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dir = '/kaggle/working/CroppedImages/train'\n","test_dir = '/kaggle/working/CroppedImages/test'\n","\n","# Using ImageDataGenerator for data augmentation\n","generator = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.1  # 10% of the data will be used for validation\n",")\n","\n","# Load and split the data into training and validation sets\n","train_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"training\"  # This is for training data\n",")\n","\n","val_ds = generator.flow_from_directory(\n","    train_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"validation\"  # This is for validation data\n",")\n","\n","# Get the list of classes\n","classes = list(train_ds.class_indices.keys())\n","print(\"Classes in training data:\", classes)\n","\n","# Load test data\n","test_ds = generator.flow_from_directory(\n","    test_dir,\n","    target_size=(image_size, image_size),\n","    batch_size=32,\n","    subset=\"training\"  # This is for test data\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Build model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-30T10:14:01.053740Z","iopub.status.idle":"2024-05-30T10:14:01.054232Z","shell.execute_reply":"2024-05-30T10:14:01.054006Z","shell.execute_reply.started":"2024-05-30T10:14:01.053985Z"},"trusted":true},"outputs":[],"source":["# Ensure TensorFlow is set to use GPU (if available)\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["IMAGE_SIZE = [image_size, image_size, 3]  # we will keep the image size as (128, 128). You can increase the size for better results.\n","\n","# loading the weights of pretrained model without the top layer. These weights are trained on Imagenet dataset.\n","# pretrained_model = VGG16(input_shape = IMAGE_SIZE, weights = 'imagenet', include_top = False)\n","# pretrained_model = ResNet50(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = DenseNet121(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = InceptionV3(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","# pretrained_model = MobileNetV2(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","pretrained_model = EfficientNetB0(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","\n","# this will exclude the initial layers from the training phase as they have already been trained.\n","for layer in pretrained_model.layers:\n","    layer.trainable = False\n","\n","x = Flatten()(pretrained_model.output)\n","x = Dense(128, activation = 'relu')(x)  # we can add a new fully connected layer but it will increase the execution time.\n","x = Dense(31, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi-label classification problem.\n","\n","model = Model(inputs = pretrained_model.input, outputs = x)\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(\"Model created\")\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","history = model.fit(train_ds, epochs=30, validation_data=val_ds, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{},"source":["## Result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss, train_accuracy = model.evaluate(train_ds)\n","print(f\"Training Accuracy: {train_accuracy*100: .2f}\")\n","print(f\"Training Loss: {train_loss: .2f}\")\n","\n","validation_loss, validation_accuracy = model.evaluate(val_ds)\n","print(f\"Validation Accuracy: {validation_accuracy*100: .2f}\")\n","print(f\"Validation Loss: {validation_loss: .2f}\")\n","\n","test_loss, test_accuracy = model.evaluate(test_ds)\n","print(f\"Test Accuracy: {test_accuracy*100: .2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n","ax = ax.ravel()\n","\n","for i, met in enumerate(['accuracy', 'loss']):\n","    ax[i].plot(history.history[met])\n","    ax[i].plot(history.history['val_' + met])\n","    ax[i].set_title('Model {}'.format(met))\n","    ax[i].set_xlabel('epochs')\n","    ax[i].set_ylabel(met)\n","    ax[i].legend(['train', 'val'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_roc_curves(model, test_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save('FRM_Crop.h5')"]},{"cell_type":"markdown","metadata":{},"source":["# Metric learning"]},{"cell_type":"markdown","metadata":{},"source":["## Metric learning CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:14:49.070454Z","iopub.status.busy":"2024-05-30T10:14:49.070056Z","iopub.status.idle":"2024-05-30T10:14:49.074907Z","shell.execute_reply":"2024-05-30T10:14:49.073867Z","shell.execute_reply.started":"2024-05-30T10:14:49.070422Z"},"trusted":true},"outputs":[],"source":["tf.config.run_functions_eagerly(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_size_dim = 128"]},{"cell_type":"markdown","metadata":{},"source":["### Build dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Data Preparation\n","data_dir = '/kaggle/input/Original Images/Original Images'\n","image_size = (image_size_dim, image_size_dim)\n","\n","def load_images(data_dir, image_size):\n","    labels = []\n","    images = []\n","    for label_dir in os.listdir(data_dir):\n","        label_path = os.path.join(data_dir, label_dir)\n","        if os.path.isdir(label_path):\n","            for image_name in os.listdir(label_path):\n","                image_path = os.path.join(label_path, image_name)\n","                image = load_img(image_path, target_size=image_size, color_mode='grayscale')\n","                image = img_to_array(image) / 255.0\n","                images.append(image)\n","                labels.append(label_dir)\n","    return np.array(images), np.array(labels)\n","\n","images, labels = load_images(data_dir, image_size)\n","label_to_int = {label: idx for idx, label in enumerate(np.unique(labels))}\n","int_labels = np.array([label_to_int[label] for label in labels])\n","\n","# Generate pairs\n","def generate_pairs(images, labels):\n","    pairs = []\n","    pair_labels = []\n","    num_classes = len(np.unique(labels))\n","    for idx1 in range(len(images)):\n","        current_image = images[idx1]\n","        label = labels[idx1]\n","        positive_idx = np.where(labels == label)[0]\n","        negative_idx = np.where(labels != label)[0]\n","        pos = images[np.random.choice(positive_idx)]\n","        neg = images[np.random.choice(negative_idx)]\n","        pairs += [[current_image, pos], [current_image, neg]]\n","        pair_labels += [1, 0]\n","    return np.array(pairs), np.array(pair_labels)\n","\n","pairs, pair_labels = generate_pairs(images, int_labels)\n","\n","# Split into training and test sets\n","(pairs_train, pairs_test, labels_train, labels_test) = train_test_split(pairs, pair_labels, test_size=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Model Architecture\n","def create_base_network(input_shape):\n","    model = models.Sequential()\n","    model.add(layers.Conv2D(64, (7, 7), activation='relu', input_shape=input_shape))\n","    model.add(layers.MaxPooling2D())\n","    model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n","    model.add(layers.MaxPooling2D())\n","    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(128))\n","    return model\n","\n","input_shape = image_size + (1,)\n","base_network = create_base_network(input_shape)\n","\n","input_a = tf.keras.Input(shape=input_shape)\n","input_b = tf.keras.Input(shape=input_shape)\n","\n","processed_a = base_network(input_a)\n","processed_b = base_network(input_b)\n","\n","distance = tf.keras.layers.Lambda(lambda embeddings: tf.keras.backend.sqrt(\n","    tf.keras.backend.sum(tf.keras.backend.square(embeddings[0] - embeddings[1]), axis=-1, keepdims=True))\n",")([processed_a, processed_b])\n","\n","model = tf.keras.Model(inputs=[input_a, input_b], outputs=distance)\n","\n","# Loss Function\n","def contrastive_loss(y_true, y_pred, margin=1):\n","    square_pred = tf.keras.backend.square(y_pred)\n","    margin_square = tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0))\n","    return tf.keras.backend.mean(y_true * square_pred + (1 - y_true) * margin_square)\n","\n","model.compile(loss=contrastive_loss, optimizer='adam')\n","\n","# Training\n","history = model.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train, \n","                    validation_data=([pairs_test[:, 0], pairs_test[:, 1]], labels_test),\n","                    epochs=30, batch_size=128)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluation\n","y_pred = model.predict([pairs_test[:, 0], pairs_test[:, 1]])\n","y_pred = np.where(y_pred < 0.5, 1, 0)\n","accuracy = accuracy_score(labels_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'Loss: {history.history[\"loss\"][-1]:.4f}')\n","\n","# Plot training & validation loss values\n","plt.figure()\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Predict distances on the test set\n","predicted_distances = model.predict([pairs_test[:, 0], pairs_test[:, 1]])\n","\n","# Compute ROC curve and AUC\n","fpr, tpr, _ = roc_curve(labels_test, predicted_distances)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","print(f\"Micro-average ROC curve area (AUC): {roc_auc:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Metric learning using pretrain model"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T10:14:56.014153Z","iopub.status.busy":"2024-05-30T10:14:56.013558Z","iopub.status.idle":"2024-05-30T10:14:56.018425Z","shell.execute_reply":"2024-05-30T10:14:56.017456Z","shell.execute_reply.started":"2024-05-30T10:14:56.014118Z"},"trusted":true},"outputs":[],"source":["image_size_dim = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T13:20:57.539205Z","iopub.status.busy":"2024-05-30T13:20:57.538405Z","iopub.status.idle":"2024-05-30T13:21:38.075040Z","shell.execute_reply":"2024-05-30T13:21:38.074139Z","shell.execute_reply.started":"2024-05-30T13:20:57.539168Z"},"trusted":true},"outputs":[],"source":["# from tensorflow.keras.applications.vgg16 import preprocess_input\n","# from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","# from tensorflow.keras.applications.densenet import preprocess_input\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","\n","# Data Preparation\n","data_dir = '/kaggle/input/Original Images/Original Images'\n","image_size = (image_size_dim, image_size_dim, 3)\n","\n","def load_images(data_dir, image_size):\n","    labels = []\n","    images = []\n","    for label_dir in os.listdir(data_dir):\n","        label_path = os.path.join(data_dir, label_dir)\n","        if os.path.isdir(label_path):\n","            for image_name in os.listdir(label_path):\n","                image_path = os.path.join(label_path, image_name)\n","                image = load_img(image_path, target_size=image_size, color_mode='rgb')\n","                image = img_to_array(image)\n","                image = preprocess_input(image)\n","                images.append(image)\n","                labels.append(label_dir)\n","    return np.array(images), np.array(labels)\n","\n","images, labels = load_images(data_dir, image_size)\n","label_encoder = LabelEncoder()\n","int_labels = label_encoder.fit_transform(labels)\n","\n","# Generate pairs\n","def generate_pairs(images, labels):\n","    pairs = []\n","    pair_labels = []\n","    num_classes = len(np.unique(labels))\n","    for idx1 in range(len(images)):\n","        current_image = images[idx1]\n","        label = labels[idx1]\n","        positive_idx = np.where(labels == label)[0]\n","        negative_idx = np.where(labels != label)[0]\n","        pos = images[np.random.choice(positive_idx)]\n","        neg = images[np.random.choice(negative_idx)]\n","        pairs += [[current_image, pos], [current_image, neg]]\n","        pair_labels += [1, 0]\n","    return np.array(pairs), np.array(pair_labels)\n","\n","pairs, pair_labels = generate_pairs(images, int_labels)\n","\n","# Split into training and test sets\n","(pairs_train, pairs_test, labels_train, labels_test) = train_test_split(pairs, pair_labels, test_size=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T13:21:38.076977Z","iopub.status.busy":"2024-05-30T13:21:38.076686Z","iopub.status.idle":"2024-05-30T13:21:39.361658Z","shell.execute_reply":"2024-05-30T13:21:39.360347Z","shell.execute_reply.started":"2024-05-30T13:21:38.076950Z"},"trusted":true},"outputs":[],"source":["# Model Architecture\n","def create_base_network(input_shape):\n","#     pretrained_model = VGG16(input_shape=input_shape, weights = 'imagenet', include_top = False)\n","#     pretrained_model = DenseNet121(input_shape=input_shape, weights='imagenet', include_top=False)\n","#     pretrained_model = InceptionV3(input_shape=input_shape, weights='imagenet', include_top=False)\n","    pretrained_model = MobileNetV2(input_shape=input_shape, weights='imagenet', include_top=False)\n","#     pretrained_model = EfficientNetB0(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n","    x = layers.Flatten()(pretrained_model.output)\n","    x = layers.Dense(128)(x)\n","    model = models.Model(inputs=pretrained_model.input, outputs=x)\n","    return model\n","\n","input_shape = image_size\n","base_network = create_base_network(input_shape)\n","\n","input_a = tf.keras.Input(shape=input_shape)\n","input_b = tf.keras.Input(shape=input_shape)\n","\n","processed_a = base_network(input_a)\n","processed_b = base_network(input_b)\n","\n","distance = tf.keras.layers.Lambda(lambda embeddings: tf.keras.backend.sqrt(\n","    tf.keras.backend.sum(tf.keras.backend.square(embeddings[0] - embeddings[1]), axis=-1, keepdims=True))\n",")([processed_a, processed_b])\n","\n","model = tf.keras.Model(inputs=[input_a, input_b], outputs=distance)\n","\n","# Loss Function\n","def contrastive_loss(y_true, y_pred, margin=1):\n","    square_pred = tf.keras.backend.square(y_pred)\n","    margin_square = tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0))\n","    return tf.keras.backend.mean(y_true * square_pred + (1 - y_true) * margin_square)\n","\n","# Compile the model with accuracy metric\n","model.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = model.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train, \n","                    validation_data=([pairs_test[:, 0], pairs_test[:, 1]], labels_test),\n","                    epochs=50, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["### Result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T14:56:54.810525Z","iopub.status.busy":"2024-05-30T14:56:54.810197Z","iopub.status.idle":"2024-05-30T14:57:06.071009Z","shell.execute_reply":"2024-05-30T14:57:06.070075Z","shell.execute_reply.started":"2024-05-30T14:56:54.810497Z"},"trusted":true},"outputs":[],"source":["y_pred = model.predict([pairs_test[:, 0], pairs_test[:, 1]])\n","y_pred = np.where(y_pred < 0.5, 1, 0)\n","accuracy = accuracy_score(labels_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'Loss: {history.history[\"loss\"][-1]:.4f}')\n","\n","# Plot training & validation accuracy values\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","# Plot training & validation loss values\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T14:57:06.073174Z","iopub.status.busy":"2024-05-30T14:57:06.072460Z","iopub.status.idle":"2024-05-30T14:57:17.064669Z","shell.execute_reply":"2024-05-30T14:57:17.063777Z","shell.execute_reply.started":"2024-05-30T14:57:06.073136Z"},"trusted":true},"outputs":[],"source":["# Predict distances on the test set\n","predicted_distances = model.predict([pairs_test[:, 0], pairs_test[:, 1]])\n","\n","# Compute ROC curve and AUC\n","fpr, tpr, _ = roc_curve(labels_test, predicted_distances)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","print(f\"Micro-average ROC curve area (AUC): {roc_auc:.2f}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":959963,"sourceId":1624149,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
